## TL;DR
- 벡터 미적분은 스칼라장·벡터장에 대한 $
abla$, $
abla\cdot$, $
abla\times$ 연산을 체계화해 역전파, Normalizing Flow, 물리 기반 모델의 수학적 토대를 제공한다.
- 야코비안과 헤시안은 다변수 체인룰, 2차 최적화, Sensitivity 분석에서 핵심 역할을 하며, PyTorch의 autograd 연산자가 이를 자동 계산한다.
- 스톡스·가우스 정리를 활용하면 공간적 적분과 미분 연산의 관계를 파악해 물리적 제약이나 보존 법칙을 만족하는 모델을 만들 수 있다.

## 언제 쓰나
- Transformer attention, Normalizing Flow, LoRA 등에서 행렬 야코비안과 행렬식이 필요한 경우
- PINN, Neural ODE, Diffusion 모델처럼 벡터장과 PDE를 직접 다루는 딥러닝 구조를 설계할 때
- 2차 최적화(Newton, Quasi-Newton, Laplace Approximation)나 Fisher 정보 행렬을 계산할 때
- Autograd가 생성한 그래디언트·헤시안의 수치적 안정성을 검증하고 해석해야 할 때

## 주요 API
| 개념/연산 | 정의 및 수식 | PyTorch/실습 포인트 |
| --- | --- | --- |
| 그래디언트 $\nabla f$ | $\nabla f(\mathbf{x}) = \left[\frac{\partial f}{\partial x_1},\ldots,\frac{\partial f}{\partial x_n}\right]^\top$ | `torch.autograd.grad`로 스칼라 출력의 기울기 획득 |
| 방향 도함수 | $D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$ | 단위 벡터로 투영한 gradient dot product |
| 야코비안 $J_{\mathbf{f}}$ | $(J_{\mathbf{f}})_{ij} = \frac{\partial f_i}{\partial x_j}$ | `torch.autograd.functional.jacobian` |
| 발산 $\nabla\cdot\mathbf{F}$ | $\sum_i \frac{\partial F_i}{\partial x_i}$ | 스칼라장을 통해 보존 여부 판단 |
| 회전 $\nabla\times\mathbf{F}$ | 3D 벡터장의 소용돌이 성분 | `torch.autograd.grad` 반복으로 구성 |
| 헤시안 $H_f$ | $(H_f)_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$ | `torch.autograd.functional.hessian` |
| 벡터-야코비안 곱 | $v^\top J$ | `torch.autograd.grad(outputs, inputs, grad_outputs=v)` |
| 야코비안-벡터 곱 | $J v$ | `forward_ad` 또는 `torch.autograd.functional.jvp` |

## 실습 예제
### 1. Autograd로 야코비안·발산·회전 계산
```python
import torch

def vector_field(x):
    # x: (batch, 3)
    y = torch.zeros_like(x)
    y[:, 0] = x[:, 1] * torch.sin(x[:, 2])
    y[:, 1] = x[:, 0] * x[:, 2]
    y[:, 2] = torch.exp(x[:, 0]) - x[:, 1]
    return y

x = torch.tensor([[0.3, -0.2, 0.5]], requires_grad=True)
y = vector_field(x)

def scalar_flux(x):
    f = torch.sum(vector_field(x)**2, dim=-1)
    return f

# 그래디언트 (스칼라장)
grad = torch.autograd.grad(scalar_flux(x), x, create_graph=True)[0]

# 발산: trace(Jacobian)
J = torch.autograd.functional.jacobian(lambda inp: vector_field(inp), x)
divergence = torch.einsum('bii->b', J)

# 회전: curl(F)
def curl_component(idx):
    axes = [(1, 2), (2, 0), (0, 1)][idx]
    return torch.autograd.grad(y[0, axes[1]], x, retain_graph=True)[0][0, axes[0]] - \
           torch.autograd.grad(y[0, axes[0]], x, retain_graph=True)[0][0, axes[1]]
curl = torch.stack([curl_component(i) for i in range(3)], dim=0)

print('grad:', grad)
print('divergence:', divergence)
print('curl:', curl)
```
- `torch.autograd.functional.jacobian`은 입력과 출력의 차원이 클 경우 비용이 크므로, 실제 프로젝트에서는 벡터-야코비안 곱을 사용해 발산을 추정한다.
- 위 예제의 수치 결과(직접 계산)는 $\nabla f(0.3,-0.2,0.5) \approx (4.33,-3.19,0.12)$, $\nabla\cdot \mathbf{F} = 0$, $\nabla \times \mathbf{F} \approx (-1.30,-1.53,0.02)$로 확인되어, 벡터장의 보존·회전 성분을 빠르게 파악할 수 있다.

### 2. 야코비안 행렬식과 헤시안으로 2차 근사
```python
import torch
from torch.autograd.functional import jacobian, hessian

f = lambda params: torch.stack([
    params[0] * torch.sin(params[1]) + params[2]**2,
    torch.exp(params[0]) - params[1] * params[2]
])

params = torch.tensor([0.2, -0.1, 0.5], requires_grad=True)
J = jacobian(f, params)
H = hessian(lambda p: f(p)[0], params)

# 2차 테일러 근사
delta = torch.tensor([0.05, -0.02, 0.01])
linear_term = J @ delta
quad_term = 0.5 * delta @ H @ delta
approx = f(params) + linear_term + quad_term

print('Jacobian:\n', J)
print('Hessian (f0):\n', H)
print('Second-order approximation:', approx)
```
- LoRA나 Adapter 설계 시 $\det(J)$ 혹은 Truncated SVD 이전에 야코비안/헤시안 기반 근사를 계산하면 민감도를 가늠할 수 있다.
- 위 코드에서 $J \approx \begin{bmatrix}-0.0998 & 0.1990 & 1.0\\ 1.2214 & -0.5 & 0.1\end{bmatrix}$, $H_{f_0}$의 비대각 성분은 $\cos(-0.1)\approx0.995$로 채워지며, $\Delta=(0.05,-0.02,0.01)$를 대입한 2차 근사는 $f(\mathbf{p}+\Delta)\approx(0.2302, 1.3426)$으로 스칼라 항의 곡률 효과가 반영된다.

## 실수 주의
- 텐서의 batch 차원을 명시하지 않으면 야코비안 결과를 해석하기 어려우므로 `(batch, dim)` 형태를 유지하고 `vectorized=True`를 고려한다.
- 큰 네트워크에서 전체 야코비안/헤시안을 직접 계산하면 $O(n^2)$ 이상의 메모리가 필요하므로, VJP/JVP를 통해 필요한 방향만 계산한다.
- 발산·회전을 수치 미분으로 계산할 때 grid spacing을 통일하지 않으면 물리적 의미가 왜곡된다.
- 스톡스 정리, 가우스 정리 적용 시 영역 경계(normal vector)를 명시하지 않으면 부호가 바뀌어 보존 조건 검증이 실패할 수 있다.

## 관련 노트
- [[Math/Calculus/3. 다변수 미적분]]
- [[Math/Calculus/5. 자동 미분]]
- [[Math/Optimization/3. 2차 최적화 기법]]
- [[Math/Linear Algebra/5. 특잇값 분해]]
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Math/Math_확장_계획]]
