## TL;DR
- 수치 선형대수는 대규모 행렬 연산의 안정성과 효율을 다루며, DEQ/Implicit Layer, Hessian-Free Optimization의 필수 전제다.
- Phase 4의 Numerical 라인업을 여는 템플릿으로, 반복법·조건수·사전조건자 실습을 담을 구조를 미리 마련했다.
- GPU/TPU 환경에서 PyTorch로 구현할 반복 솔버, SciPy 비교 실험, 수렴 곡선 시각화 자리까지 예약한다.

## 언제 쓰나
- 거대 모델 학습에서 Hessian-vector product를 근사하거나, Conjugate Gradient로 선형 시스템을 푸는 경우
- Deep Equilibrium Model, Implicit Layer에서 고정점 방정식을 반복적으로 풀어야 할 때
- Preconditioning을 통해 학습 속도를 개선하거나, 대규모 유한요소(FEM) 시스템을 해석할 때

## 주요 API
| 이름 | 설명 | 예정 실습 |
| --- | --- | --- |
| `torch.linalg.cond` | 조건수 계산으로 수치 안정성 평가 | 서로 다른 스케일링 전략 비교 |
| `torch.linalg.solve` | 직접 해법 (LU 기반) | LU vs. CG 성능 비교 |
| `torch.cuda.amp.autocast` | 혼합 정밀도, 수치 오차 분석용 | FP16 반복법 실험 |
| `scipy.sparse.linalg.cg` | Conjugate Gradient 레퍼런스 구현 | PyTorch 커스텀 CG와 결과 비교 |

## 실습 예제
1. 무작위 SPD 행렬을 생성해 `torch.linalg.cond`로 조건수를 측정하고, 정규화/스케일링 전후 변화를 표로 정리 (작성 예정).
2. PyTorch 기반 Conjugate Gradient 구현을 작성하고 SciPy 버전과 수렴 곡선 및 시간 복잡도를 비교 (작성 예정).
3. Preconditioner (Jacobi, ILU 등) 적용 전후의 반복 횟수 차이를 시각화하고, Implicit Layer 학습 루프에 통합 (작성 예정).

## 실수 주의
- SPD가 아닌 행렬에 CG를 적용하면 수렴이 보장되지 않으므로, 대칭성/양의 정부호 조건을 먼저 확인한다.
- 혼합 정밀도 사용 시 스케일링이 잘못되면 언더플로/오버플로로 NaN이 발생하므로 gradient clipping 전략을 병행한다.
- 사전조건자를 구현할 때 역행렬을 직접 계산하면 수치적으로 불안정하고 비용이 증가한다.

## 관련 노트
- [[Math/Linear Algebra/5. 특잇값 분해]]
- [[Math/Linear Algebra/6. 슈타이니츠 교환 정리]]
- [[Math/Optimization/3. 2차 최적화 기법]]
- [[Foundations/3. 최적화]]
- [[Math/Math_확장_계획]]
