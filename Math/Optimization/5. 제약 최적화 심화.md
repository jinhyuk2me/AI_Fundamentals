## TL;DR
- 제약 최적화는 라그랑주 승수와 KKT 조건으로 최적성(Stationarity, Primal/Dual feasibility, Complementary slackness)을 정량화한다.
- 쌍대 문제와 강/약쌍대성은 SVM, Lasso, Fairness-Constrained ML에서 손실과 제약을 바꾸어 푸는 핵심 도구다.
- Proximal Operator, Projected Gradient, ADMM은 비분리/비평활 제약을 다루기 위한 실무 알고리즘이다.

## 언제 쓰나
- L1/L2 혼합 정규화, Sparsity, Positive Semidefinite 등 구조적 제약을 가진 모델을 학습할 때
- Support Vector Machine, Portfolio Optimization, Resource Allocation 등 쌍대 해석이 필요한 문제를 다룰 때
- GAN/Optimal Transport처럼 프라이멀·쌍대 해를 번갈아 업데이트하는 알고리즘을 구현할 때
- Diffusion/Score 기반 모델에서 Proximal gradient를 사용해 노이즈 제약이나 clipping을 enforcing할 때

## 주요 API
| 개념 | 정의 | 실습 포인트 |
| --- | --- | --- |
| 라그랑지안 $\mathcal{L}(x,\lambda,\nu)$ | $f(x)+\sum_i\lambda_i g_i(x)+\sum_j\nu_j h_j(x)$ | Stationarity/Feasibility 확인 |
| KKT 조건 | Stationarity, Primal, Dual, Complementary slackness | 강쌍대성 하에서 필요·충분 |
| 쌍대 함수 $q(\lambda, \nu)$ | $\inf_x \mathcal{L}(x,\lambda,\nu)$ | SVM, Lasso, OT에 사용 |
| Prox 연산자 | $\text{prox}_{\lambda g}(v)=\arg\min_x g(x)+\frac{1}{2\lambda}\|x-v\|^2$ | L1 soft-thresholding |
| Projected Gradient | $x_{k+1}=\Pi_{\mathcal{C}}(x_k-\alpha_k \nabla f(x_k))$ | 단순 bound 제약 |
| ADMM | $x$-$z$ 분할로 Alternating + Dual update | $
abla f$ 계산과 Prox 분리 |
| Barrier/Penalty | 제약을 로그/벌점으로 흡수 | Practical interior-point |

## 실습 예제
### 1. KKT 조건 검증 (Quadratic + Equality 제약)
```python
import math

# minimize f(x,y) = (x-1)^2 + (y-2)^2 s.t. x + y = 1
# 라그랑지안: L = (x-1)^2 + (y-2)^2 + λ (x + y - 1)

lambda_val = -2.0
x_opt = 1.5
y_opt = -0.5

stationarity_x = 2*(x_opt-1) + lambda_val
stationarity_y = 2*(y_opt-2) + lambda_val
print('stationarity:', stationarity_x, stationarity_y)
print('primal feasibility:', x_opt + y_opt - 1)
```
- 출력: `stationarity: 0.0 0.0`, `primal feasibility: 0.0`. equality 제약에서는 complimentary slackness가 자동 충족된다.
- 간단한 예라도, Stationarity + 제약 만족 여부를 확인하면 KKT를 수작업으로 검증할 수 있다.

### 2. Projected Gradient Descent (Box 제약)
```python
import random

# minimize f(x) = (x-3)^2 subject to 0 <= x <= 2
x = 0.0
alpha = 0.2
for _ in range(20):
    grad = 2*(x-3)
    x = x - alpha * grad
    x = max(0.0, min(2.0, x))  # projection onto [0,2]
print('PGD solution:', x)
```
- unconstrained minimizer는 3이지만, 박스 제약으로 인해 최적해는 2에서 발생한다.
- RL/Control에서 action clipping을 PGD 관점으로 이해하면, 안전 제약을 구현할 때 gradient 흐름을 분석할 수 있다.

### 3. Soft-thresholding Prox로 L1 제약 해결
```python
import math

v = [3.0, -1.2, 0.5]
lam = 0.8
prox = [math.copysign(max(abs(val) - lam, 0.0), val) for val in v]
print('prox result:', prox)
```
- Proximal operator는 $\ell_1$ penalty에 대한 closed-form soft-thresholding을 제공한다.
- Lasso, ISTA/FISTA, Sparse Autoencoder 등의 업데이트를 구현할 때 미분 가능한 대체 없이도 정리된 솔루션을 얻는다.

### 4. 2-Block ADMM 스케치
```python
# minimize 0.5||x||^2 + λ||z||_1 subject to x - z = 0
# ADMM updates:
# x^{k+1} = argmin 0.5||x||^2 + (ρ/2)||x - z^k + u^k||^2 -> closed form (averaging)
# z^{k+1} = soft_threshold(x^{k+1} + u^k, λ/ρ)
# u^{k+1} = u^k + x^{k+1} - z^{k+1}
```
- 실제 구현에서는 $x$ 업데이트가 선형 시스템 풀이로 바뀌고, $z$ 업데이트는 prox 연산으로 처리된다.
- ADMM은 분산 학습/프라이버시 제약/분리된 모델 학습에서 널리 사용된다.

## 실수 주의
- KKT 조건은 convex + constraint qualification(CQ)이 충족될 때만 충분조건이므로, LICQ/Slater 조건을 반드시 확인한다.
- Duality gap이 0이 아니면 쌍대 해만으로 프라이멀 해를 복원할 수 없다. 강쌍대성이 깨지는 문제(비볼록 등)는 별도 조치 필요.
- PGD/ADMM에서 step size나 penalty가 너무 크면 발산하거나 진동하므로, Lipschitz 상수 기반으로 조정하거나 backtracking을 사용한다.
- Prox 연산자가 잘못 구현되면 sparsity가 유지되지 않거나 constraint가 깨져 downstream 모델이 무의미해질 수 있다.
- Barrier method에서 μ를 너무 빨리 줄이면 Newton step이 수렴하지 않고, 너무 느리게 줄이면 계산 비용이 폭증한다.

## 관련 노트
- [[Math/Optimization/3. 2차 최적화 기법]]
- [[Math/Optimization/4. 볼록성과 제약 최적화]]
- [[Math/Linear Algebra/7. 저랭크 근사와 행렬 분해]]
- [[Math/Calculus/6. 벡터 미적분]]
- [[Foundations/3. 최적화]]
- [[Math/Math_확장_계획]]
