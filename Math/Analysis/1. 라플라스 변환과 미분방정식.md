## TL;DR
- 라플라스 변환 $\mathcal{L}\{f(t)\}(s)=\int_0^\infty e^{-st}f(t)\,dt$ 는 선형 미분방정식을 다항식 대수 문제로 변환해 초기값 조건을 손쉽게 포함시키는 도구다.
- Neural ODE, Neural Laplace, Laplace Approximation, Bayesian DL, PINN에서 상태공간을 안정적으로 해석하고 불확실성을 정량화하는 핵심 수학이다.
- 벡터장/미분방정식을 다루는 모든 딥러닝 응용에서 ROC, 극점/영점, 변환 표를 정확히 이해해야 수치적 함정을 피할 수 있다.

## 언제 쓰나
- Transformer, PINN, Neural ODE 등 동적 시스템을 모델링할 때 초기값 문제를 안정적으로 풀고자 할 때
- Laplace Approximation을 이용한 베이지안 추론 및 불확실성 추정에서 2차 정규근사를 구성할 때
- Differential equation 기반 제어 시뮬레이션을 PyTorch로 구현하고 학습 루프에 통합할 때

## 주요 API
| 이름 | 설명 | 예정 실습 |
| --- | --- | --- |
| `\mathcal{L}\{f(t)\}` | $f(t)$를 복소평면 $s$-영역으로 사상하는 정방향 라플라스 변환 | `sympy.integrals.transforms.laplace_transform` 사용 예제 |
| `\mathcal{L}^{-1}\{F(s)\}` | 라플라스 역변환, 부분 분수/복소 적분으로 시간 영역 복원 | 역변환 표와 수치 역변환 실습 |
| `\text{LaplaceApprox}` | PyTorch 모형의 Post-hoc Bayesian 추정을 위한 Laplace Redux 함수 (예: `laplace.torch`) | VGG/MLP에 Laplace 적용, 불확실성 계산 |
| `torchdiffeq.odeint` | Neural ODE 구현 시 라플라스 기반 초기값 문제를 검증하는 비교 군 | 라플라스 + ODE 솔버 결과 비교 |

## 핵심 개념

### 라플라스 변환과 ROC
- 정의: $\mathcal{L}\{f(t)\}(s) = \int_{0}^{\infty} e^{-st}f(t)\,dt$ (복소수 $s = \sigma + i\omega$).
- 수렴 영역(ROC): 실수부 $\sigma$가 특정 임계값보다 클 때만 적분이 수렴한다. 예: $f(t)=e^{at}$의 ROC는 $\sigma > a$.
- 선형성: $\mathcal{L}\{af+bg\}=a\mathcal{L}\{f\}+b\mathcal{L}\{g\}$, 미분은 $t$-영역의 미분을 $s$-영역 다항식으로 변환 (아래 참조).

### 미분/적분 성질
- 1차 미분: $\mathcal{L}\{f'(t)\}=sF(s) - f(0^+)$.
- 2차 이상: $\mathcal{L}\{f''(t)\}=s^2F(s) - sf(0^+) - f'(0^+)$.
- 적분: $\mathcal{L}\{\int_0^t f(\tau)d\tau\} = \frac{1}{s}F(s)$.
- 지연/이동: $\mathcal{L}\{f(t-a)u(t-a)\}=e^{-as}F(s)$ (단위 계단 함수 $u$ 포함).

### 역변환과 표
- 단순 극점: $F(s)=\frac{1}{s-a}$ → $f(t)=e^{at}$.
- 부분 분수 분해를 통해 복잡한 $F(s)$를 표에 있는 기본 항들의 조합으로 나눠 역변환.
- 복소수 극점은 사인/코사인 항을 생성한다: $\mathcal{L}^{-1}\{\frac{\omega}{(s-a)^2+\omega^2}\}=e^{at}\sin\omega t$.

### ODE/시스템 해법
1. 미분방정식 양변에 라플라스 변환 적용.
2. 초기조건을 대입해 $F(s)$를 대수적으로 풂.
3. 부분 분수/표를 사용해 역변환하여 시간영역 해를 얻음.
4. 교차 검증: 해를 미분해 원래 ODE에 대입해 검산.

### 컨볼루션과 임펄스 응답
- $\mathcal{L}\{f*g\} = F(s)G(s)$ → 임펄스 응답/시스템 함수를 곱으로 처리하여 합성곱을 단순화.
- 신경 PDE, 필터 설계, Normalizing Flow에서 특정 커널을 주파수-라플라스 영역으로 옮겨 계산량을 줄인다.

## 최신 응용 맥락

### Neural Laplace (ICML 2022)
- 연속 확산계, PDE, event-driven ODE를 라플라스 영역에서 직접 학습하는 프레임워크.
- 시간-영역 신호 대신 $s$-영역 표현을 예측해 고주파/강 비선형 동역학을 안정적으로 학습.
- 복소수 입력을 다루기 위해 real-imaginary 분해 또는 Wirtinger calculus를 사용하며, ROC 내 샘플링 전략이 학습 성패를 좌우한다.

### Laplace Approximation for Bayesian DL
- 사후 분포 $p(\theta \mid D)$를 최적점 $\hat{\theta}$ 주변 2차 근사: $p(\theta \mid D) \approx \mathcal{N}(\hat{\theta}, H^{-1})$, $H$는 음의 로그 사후의 헤시안.
- Laplace Redux, SWAG, KFAC-Laplace 등은 대규모 신경망에서도 근사 가능한 구조(블록 대각, Kronecker factored)를 사용한다.
- Optimizer 스텝과 라플라스 하이퍼파라미터(precision)를 공동 최적화해 불확실성 추정 품질을 높인다.

### Neural ODE / PINN 연계
- 라플라스 변환은 선형 부분을 닫힌형으로 처리하고, 비선형 잔차만 학습하도록 분리하는 Hybrid Neural ODE 설계에 쓰인다.
- PINN에서 초기값/경계조건이 라플라스 영역에서 다항식으로 표현되면 손실 함수가 단순해져 학습 안정화에 기여한다.

## 실습 예제
### 1. 수치 적분으로 라플라스 변환 근사
```python
import math

def laplace_numeric(f, s, upper=50.0, steps=200_000):
    dt = upper / steps
    total = 0.0
    for i in range(steps + 1):
        t = i * dt
        weight = 0.5 if i in (0, steps) else 1.0
        total += weight * math.exp(-s * t) * f(t)
    return total * dt

f = lambda t: t * math.exp(-t)          # 대상 함수
s = 1.5                                 # 평가할 영역
approx = laplace_numeric(f, s)
closed = 1.0 / (s + 1.0)**2             # 해석적 값
print(f"numeric={approx:.10f}, closed-form={closed:.10f}, abs err={abs(approx-closed):.2e}")
```
- 출력: `numeric=0.1599999948, closed-form=0.1600000000, abs err=5.21e-09`.
- 수치 적분으로도 간단한 함수의 라플라스 변환을 안정적으로 근사할 수 있으며, ROC 내에서 적분 상한과 스텝을 조정하면 오차를 제어할 수 있다.

### 2. 라플라스 변환 기반 2차 ODE 해 검증
```python
import math

def y(t):        # 라플라스 변환으로 얻은 해: y(t) = e^{-t} + t e^{-t}
    return math.exp(-t) + t * math.exp(-t)

def y_prime(t):
    return -t * math.exp(-t)

def y_double(t):
    return -math.exp(-t) + t * math.exp(-t)

for t in [0.0, 0.5, 1.0, 1.5, 2.0]:
    lhs = y_double(t) + 3 * y_prime(t) + 2 * y(t)
    rhs = math.exp(-t)
    print(f"t={t:.1f}, residual={lhs - rhs:+.2e}")
```
- 방정식 $y'' + 3y' + 2y = e^{-t}$, $y(0)=1$, $y'(0)=0$에 대해 라플라스 영역에서 $Y(s)$를 구하고 역변환하면 $y(t)=e^{-t}+t e^{-t}$.
- 위 검증 코드의 residual은 $10^{-16}$ 수준이라 해가 정확히 방정식을 만족함을 확인할 수 있다.

### 3. 라플라스 기반 시스템 응답 스케치
- Linear Time-Invariant 시스템의 전달함수 $G(s)=\frac{1}{(s+1)(s+3)}$를 부분 분수로 분해하면 $g(t)=\frac{1}{2}(e^{-t}-e^{-3t})$.
- 신경 ODE/제어기 구현 시, 학습된 모델이 생성한 $G(s)$와 이론적 전이함수를 비교하면 안정성(극점 위치)과 응답 속도를 진단할 수 있다.
- 실제 프로젝트에서는 `torchdiffeq`로 시간 영역 응답을 적분하고, 위와 같은 분석적 표준 응답과 MSE를 비교하여 학습 품질을 측정한다.

## 실수 주의
- 라플라스 변환의 수렴 영역(ROC)을 명시하지 않으면 Neural Laplace 논문과의 비교에서 오류가 발생한다.
- $s$-평면에서의 극점/영점을 잘못 해석하면 역변환 시 허수부 부호가 뒤집히므로 복소수 표기를 철저히 관리해야 한다.
- 수치 역라플라스 구현 시 FFT 해상도와 샘플 시간을 일치시키지 않으면 PyTorch 시뮬레이션과 불일치가 생길 수 있다.
- 수치 적분으로 라플라스 변환을 계산할 때 상한(upper)을 충분히 크게 두지 않으면 tail 적분이 무시되어 큰 오차가 발생한다.
- 딥러닝 모델에 Laplace Approximation을 적용할 때, 헤시안 근사를 잘못 선택하면 음의 고유값으로 인해 분산이 음수가 되므로 (예: KFAC/L-BFGS) SPD 보정이 필요하다.

## 관련 노트
- [[Math/Calculus/2. 단변수 미적분]]
- [[Math/Calculus/4. 테일러 급수와 최적화]]
- [[Math/Optimization/3. 2차 최적화 기법]]
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Math/Probability/5. 베이지안 추론]]
- [[Math/Math_확장_계획]]
