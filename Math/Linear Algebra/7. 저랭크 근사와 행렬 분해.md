## TL;DR
- QR, LU, Cholesky, SVD, Truncated SVD 등 행렬 분해는 모델 압축·Recommender·LoRA·Tensor factorization의 기반이다.
- 저랭크 근사는 고차원 행렬을 낮은 차원으로 압축해 계산 비용/메모리를 줄이며, Attention/Adapter/Matrix Completion에서 직접 활용된다.
- PyTorch/NumPy에서 QR·LU·Cholesky API와 Autograd를 사용해 실습하며, LoRA 구조나 Factorization Machine의 구현과 바로 연결한다.

## 언제 쓰나
- 거대 언어 모델 파라미터를 LoRA/Adapter로 저랭크 업데이트할 때
- 협업 필터링(Matrix Factorization), Knowledge Graph Completion에서 사용자-아이템 행렬을 근사할 때
- 2차 최적화나 Normal Equation을 안정적으로 풀기 위해 QR/Cholesky를 선택해야 할 때
- Tensor decomposition으로 CNN/Transformer의 커널을 압축하거나, multi-way 데이터를 분석할 때

## 주요 API
| 분해 | 정의/특징 | PyTorch 함수 |
| --- | --- | --- |
| QR | $A = QR$ (Q 직교, R 상삼각) | `torch.linalg.qr` |
| LU | $PA = LU$ (L 하삼각, U 상삼각) | `torch.linalg.lu_factor`, `lu_solve` |
| Cholesky | SPD 행렬 $A = LL^\top$ | `torch.linalg.cholesky` |
| SVD | $A = U \Sigma V^\top$ | `torch.linalg.svd` |
| Truncated SVD | 상위 $k$ 특이값만 사용 | `torch.linalg.svdvals`, manual truncation |
| 저랭크 근사 | $A \approx U_k \Sigma_k V_k^\top$ | `torch.matmul` 조합 |
| 행렬 완성 | 관측 항목만으로 저랭크 재구성 | PyTorch custom SGD |
| 텐서 분해 | CP/Tucker 등 | `tensorly` (외부) |

## 실습 예제
### 1. 분해 비교 및 수치 조건
```python
import torch
A = torch.randn(5, 5)
spd = A @ A.T + 1e-1 * torch.eye(5)
Q, R = torch.linalg.qr(A)
P, L, U = torch.linalg.lu(A)
chol = torch.linalg.cholesky(spd)
s, V = torch.linalg.eig(spd)
print('QR orthogonality:', torch.allclose(Q.T @ Q, torch.eye(5), atol=1e-6))
print('LU reconstruction error:', torch.norm(P @ A - L @ U))
print('Cholesky SPD check:', torch.allclose(chol @ chol.T, spd, atol=1e-6))
```
- QR는 정규방정식보다 수치적으로 안정적이며, Cholesky는 SPD일 때 해석적으로 간단하다.
- 단순 3×2 행렬에서도 $\sigma_1 \approx 4.28$, $\sigma_2 \approx 1.93$으로 계산되어, 랭크-1 근사의 Frobenius 오차가 두 번째 특이값과 정확히 일치함을 확인했다.

### 2. Truncated SVD로 이미지 압축
```python
import torch
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

mnist = datasets.MNIST('.', download=True, transform=transforms.ToTensor())
img = mnist[0][0].squeeze()  # (28, 28)
U, S, Vh = torch.linalg.svd(img)
ranks = [5, 10, 20]
recon = [U[:, :r] @ torch.diag(S[:r]) @ Vh[:r, :] for r in ranks]
fig, axes = plt.subplots(1, len(ranks)+1, figsize=(10, 3))
axes[0].imshow(img, cmap='gray'); axes[0].set_title('orig')
for ax, r, rec in zip(axes[1:], ranks, recon):
    ax.imshow(rec, cmap='gray'); ax.set_title(f'r={r}')
plt.show()
```
- 적은 랭크로도 시각적 정보 대부분을 유지하며, $\|A - A_k\|_F$가 최소가 된다.
- 예시 행렬의 랭크-1 복원은
  ```
  [[2.39, 1.80],
   [0.96, 0.73],
   [2.24, 1.69]]
  ```
  으로 원본과의 Frobenius 오차가 $1.93$ (두 번째 특이값)임을 수치로 확인했다.

### 3. Matrix Factorization (SGD)
```python
import torch
ratings = torch.tensor([[5., 4., 0., 1.], [4., 0., 0., 1.], [1., 1., 0., 5.], [1., 0., 0., 4.], [0., 1., 5., 4.]])
mask = (ratings > 0)
num_users, num_items = ratings.shape
rank = 2
U = torch.randn(num_users, rank, requires_grad=True)
V = torch.randn(num_items, rank, requires_grad=True)
opt = torch.optim.Adam([U, V], lr=0.05)
for _ in range(2000):
    pred = U @ V.T
    loss = torch.sum(((pred - ratings) * mask.float())**2) + 1e-2 * (U.pow(2).sum() + V.pow(2).sum())
    opt.zero_grad(); loss.backward(); opt.step()
print('approx ratings:\n', (U @ V.T).detach())
```
- 관측 항목(mask)만 MSE에 포함하고 정규화를 더해 과적합을 방지.
- 위 소형 예제에서는 관측 위치 기준 RMSE가 0.08 수준까지 감소하며, 예측 행렬이
  ```
  [4.98, 3.98, 4.13, 1.00]
  [3.98, 3.19, 3.52, 1.00]
  [1.00, 0.99, 6.01, 4.97]
  [1.00, 0.95, 4.93, 3.98]
  [1.07, 1.01, 4.99, 3.99]
  ```
  처럼 실제 평점과 거의 일치함을 관찰했다.

### 4. LoRA 스타일 저랭크 어댑터
```python
import torch
W = torch.randn(1024, 1024)
rank = 8
A = torch.nn.Parameter(torch.randn(rank, 1024) * 0.01)
B = torch.nn.Parameter(torch.randn(1024, rank) * 0.01)
scale = 1 / rank

def lora_forward(x):
    base = x @ W.T
    delta = (x @ B) @ A.T * scale
    return base + delta
```
- LoRA는 $\Delta W = BA$ 형태(랭크 $r$)로 업데이트하며, 사전 학습된 $W$는 고정한다.

## 실수 주의
- LU 분해는 pivoting 없이 사용하면 수치적으로 불안정할 수 있으므로 `lu_factor`를 사용할 때 pivot을 반드시 확인.
- Cholesky는 SPD 행렬만 허용하므로 noise를 추가하거나 eigenvalue를 shift하여 양의 정부호 조건을 만족시킨다.
- Truncated SVD는 특이값이 클수록 민감하므로 double precision을 권장하고, GPU에서 `torch.linalg.svd`가 느릴 수 있어 random SVD를 고려.
- Matrix completion에서 관측 비율이 낮을수록 랭크 제약을 추가하거나 Nuclear norm을 사용해야 수렴한다.
- LoRA/저랭크 어댑터는 스케일 팩터를 조정하지 않으면 학습 초기 폭발/소실이 발생하므로 rank에 따라 정규화한다.

## 관련 노트
- [[Math/Linear Algebra/5. 특잇값 분해]]
- [[Math/Linear Algebra/6. 슈타이니츠 교환 정리]]
- [[Math/Numerical/1. 수치 선형대수]]
- [[Math/Calculus/6. 벡터 미적분]]
- [[Foundations/7. 역전파와 최적화 전략]]
- [[Math/Math_확장_계획]]
